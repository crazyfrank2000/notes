# Machine Learning

[TOC]

## 基础概念

![截屏2021-10-01 下午5.51.21](/Users/frankmac/Desktop/截屏2021-10-01 下午5.51.21.png)



## 历史

![image-20211001174853603](/Users/frankmac/Library/Application Support/typora-user-images/image-20211001174853603.png)

### 有监督学习

有监督学习通过训练样本学习得到一个模型，然后用这个模型进行推理

![img](https://pic2.zhimg.com/80/v2-3fe747d983b3fb1bbb2cacf3d7e30381_1440w.jpg)

### 无监督学习

#### - 聚类

无监督学习则没有训练过程，给定一些样本数据，让机器学习算法直接对这些数据进行分析，得到数据的某些知识

![img](https://pic3.zhimg.com/80/v2-3070edf31bd6ff3b429e10d57816c7fe_1440w.jpg)



#### - 数据降维

![img](https://pic3.zhimg.com/80/v2-d817808c633d9499daa29eff1f9ddeaa_1440w.jpg)





### 强化学习



![preview](https://pic3.zhimg.com/v2-fdd03ba3fdab788e95ee84b26922b582_r.jpg)



### 概率图模型

![img](https://pic2.zhimg.com/80/v2-4961e9ac586e8a4cf506b4d9ce73979d_1440w.jpg)



### 深度学习

![img](https://pic4.zhimg.com/80/v2-3571fbafd00cd194bf81318b1a14780b_1440w.jpg)



从学科发展过程的角度思考机器学习，有助于理解目前层出不穷的各类机器学习算法。机器学习的大致演变过程如表1所示。

| 机器学习阶段 | 年份 | 主要成果       | 代表人物                                                     |
| ------------ | ---- | -------------- | ------------------------------------------------------------ |
| 人工智能起源 | 1936 | 自动机模型理论 | 阿兰•图灵（Alan Turing）                                     |
|              | 1943 | MP模型         | 沃伦•麦卡洛克（Warren McCulloch）、沃特•皮茨（Walter Pitts） |
|              | 1951 | 符号演算       | 冯• 诺依曼（John von Neumann）                               |
|              | 1950 | 逻辑主义       | 克劳德•香农（Claude Shannon）                                |
|              | 1956 | 人工智能       | 约翰•麦卡锡（John McCarthy）、马文•明斯基（Marvin Minsky )、 克劳德•香农（Claude Shannon） |

| 人工智能初期 | 1958 | LISP                | 约翰•麦卡锡（John McCarthy）                              |
| ------------ | ---- | ------------------- | --------------------------------------------------------- |
|              | 1962 | 感知器收敛理论      | 弗兰克•罗森布拉特（Frank Rosenblatt）                     |
|              | 1972 | 通用问题求解（GPS） | 艾伦•纽厄尔（Allen Newell）、赫伯特•西蒙（Herbert Simon） |
|              | 1975 | 框架知识表示        | 马文•明斯基（Marvin Minsky）                              |



| 进化计算 | 1965 | 进化策略 | 英格•雷森博格（Ingo Rechenberg )       |
| -------- | ---- | -------- | -------------------------------------- |
|          | 1975 | 遗传算法 | 约翰•亨利•霍兰德（John Henry Holland） |
|          | 1992 | 基因计算 | 约翰•柯扎（John Koza）                 |

| 专家系统和知识工程 | 1965 | 模糊逻辑、模糊集 | 拉特飞•扎德（Lotfi Zadeh）                                   |
| ------------------ | ---- | ---------------- | ------------------------------------------------------------ |
|                    | 1969 | DENDRA、MYCIN    | 费根鲍姆（Feigenbaum )、布坎南（Buchanan )、莱德伯格（Lederberg） |
|                    | 1979 | ROSPECTOR        | 杜达（Duda）                                                 |

| 神经网络 | 1982 | Hopfield 网络   | 霍普菲尔德（Hopfield）                                       |
| -------- | ---- | --------------- | ------------------------------------------------------------ |
|          | 1982 | 自组织网络      | 图沃•科霍宁（Teuvo Kohonen）                                 |
|          | 1986 | BP算法          | 鲁姆哈特（Rumelhart）、麦克利兰（McClelland）                |
|          | 1989 | 卷积神经网络    | 乐康（LeCun）                                                |
|          | 1998 | LeNet           | 乐康（LeCun）                                                |
|          | 1997 | 循环神经网络RNN | 塞普•霍普里特（Sepp Hochreiter）、尤尔根•施密德胡伯（Jurgen Schmidhuber） |

| 分类算法 | 1986 | 决策树ID3算法 | 罗斯•昆兰（Ross Quinlan）                                  |
| -------- | ---- | ------------- | ---------------------------------------------------------- |
|          | 1988 | Boosting 算法 | 弗罗因德（Freund）、米迦勒•卡恩斯（Michael Kearns）        |
|          | 1993 | C4.5算法      | 罗斯•昆兰（Ross Quinlan）                                  |
|          | 1995 | AdaBoost 算法 | 弗罗因德（Freund）、罗伯特•夏普（Robert Schapire）         |
|          | 1995 | 支持向量机    | 科林纳•科尔特斯（Corinna Cortes）、万普尼克（Vapnik）      |
|          | 2001 | 随机森林      | 里奥•布雷曼（Leo Breiman）、阿黛勒• 卡特勒（Adele Cutler ) |

| 深度学习 | 2006 | 深度信念网络    | 杰弗里•希尔顿（Geoffrey Hinton） |
| -------- | ---- | --------------- | -------------------------------- |
|          | 2012 | 谷歌大脑        | 吴恩达（Andrew Ng）              |
|          | 2014 | 生成对抗网络GAN | 伊恩•古德费洛（Ian Goodfellow）  |

机器学习的发展分为知识推理期、知识工程期、浅层学习（Shallow Learning）和深度学习（Deep Learning）几个阶段。

## 算法

**分类**算法-是什么？即根据一个样本预测出它所属的类别。

**回归**算法-是多少？即根据一个样本预测出一个数量值。

**聚类**算法-怎么分？保证同一个类的样本相似，不同类的样本之间尽量不同。

**强化**学习-怎么做？即根据当前的状态决定执行什么动作，最后得到最大的回报。



## 框架

In this article, we take a high-level look at the major ML frameworks ones—and some newer ones to the scene:

- [TensorFlow](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref1)
- [PyTorch](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref2)
- [scikit-learn](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref3)
- [Spark ML](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref4)
- [Torch](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref5)
- [Huggingface](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref6)
- [Keras](https://www.bmc.com/blogs/machine-learning-ai-frameworks/#ref7)

![img](https://s7280.pcdn.co/wp-content/uploads/2020/09/top-machine-learning-frameworks.jpg.optimal.jpg)



## 神经网络

搭建神经网络

- 准备
- 前向传播
- 反向传播
- 迭代



正则化缓解过拟合

正则化在损失函数中引入模型复杂渎的指标，利用给W加权值，弱化了训练数据的噪声（一般不正则化b）

loss= loss(y 与y_) + regularizer * loss(w)



## Tensorflow Certification

Tensor 张量：多维数组；阶：张量的维数

0-D scalar

1-D vector

2-D matrix

n-D n-tensor

## 自然语言理解和交流

![image-20211004081011746](/Users/frankmac/Library/Application Support/typora-user-images/image-20211004081011746.png)

BERT

Pretrain Model

Fine train



## 前沿方向

- 机器视觉
- 认知与推理
- 机器人学
- 博弈与伦理
- 机器人学习

OpenAI GPT3 

common sense



## 简介

![image-20211003150203115](/Users/frankmac/Library/Application Support/typora-user-images/image-20211003150203115.png)





![image-20211003150255684](/Users/frankmac/Library/Application Support/typora-user-images/image-20211003150255684.png)



![image-20211003150409710](/Users/frankmac/Library/Application Support/typora-user-images/image-20211003150409710.png)

而自动找出事件特征与结果之间的关联模型，而变成一个能预测未来数值或者自动分类语句。‍‍角色的城市关于预测数值一个很直觉的想法，就是找出事件特征与结果之间的数学线性关系。举例来说，假设在某个地段有一间食品的房子一‍‍千万成交，那一间20平的房子以2000万成交，根据这样的资讯，我们就能合理推断出成交价与平数之间大约就是每平100万的关系‍。而当成交资讯越来越多时，我们也能利用梯度下降gradient之类的技巧找出一条，符合所有资料的回归线，并要获得一个用评述来预测房价的模型，这就是所谓的线性回归法。‍

![image-20211003150435037](/Users/frankmac/Library/Application Support/typora-user-images/image-20211003150435037.png)



![image-20211003150459643](/Users/frankmac/Library/Application Support/typora-user-images/image-20211003150459643.png)



关于自动分类则有许多做法，在此我们列举几个有名的演算法来感受一下类似于刚刚介绍的线性回归法，面对非此即彼的分类问题，‍‍我们也可以把特征与结果之间的关联投射回归到一个零与一的逻辑曲线上，零代表其中一类，一代表另外一类，‍‍如此就能利用类似的做法得到一个把任意数值对应到适当分类的模型，这就是所谓的逻辑回归法，logic requation，‍‍

决策树decision，tree是利用特征与分类结果之间的关系，有历史资料建构出一颗充满着如果这样就那样的决策树，成为一个让不同特征落入对应的适当分类的模型。‍‍而面对同样的问题，为了避免单一特征的重要性被过度放大而造成偏差，如果随机挑选部分特征来建构多科决策书，最后再利用投票的方式来决策政府‍‍1米9票‍‍多的银票少的书就这么简单，将会得出比单一决策数更全面更正确的答案，这就是随机森林法random forest‍‍类似



的概念更进一步，如果策略性逐步建构多颗决策树模型，间接量重要的特征占有更多权重，得到准确度更高的决策，树森林只是梯度提升，法规定不是t提升，‍‍c选tree或简称GB dt。‍‍最近邻K nearest neighbors或，对于想预测的新资料直接比对特征最接近的KB比历史资料，看他们分‍‍别属于哪个分类，再以投票来决定新资料的所属分类，选举最大的，秘密就票多的银票少的书就这么简单，下面一位。‍‍好傻好先生的单纯被试分类器拿一杯classify是在与其特征之间相互独立会不会影响的前提下，‍‍我们就能利用被试地理算出个别特征与结果之间的几率关系，以此来预测不同特征条件落入不同分类的个别几率，‍‍支持向量机support vector machines，huo简称SB Emme，甚至在不同分类群体之间找出一条分隔线，使边界距离最近的资料点越远越好。‍‍一起来达到分类的目的，以上都是在历史资料有标准答案的情形下，这个找出符合特征与结果之间关联性的模型，如此一来新资料就能套用相同的模型而得出适当的预测结果。

![截屏2021-10-03 下午3.05.11](/Users/frankmac/Desktop/截屏2021-10-03 下午3.05.11.png)







![截屏2021-10-03 下午3.05.29](/Users/frankmac/Desktop/截屏2021-10-03 下午3.05.29.png)



![截屏2021-10-03 下午3.06.32](/Users/frankmac/Desktop/截屏2021-10-03 下午3.06.32.png)

![截屏2021-10-03 下午3.06.58](/Users/frankmac/Desktop/截屏2021-10-03 下午3.06.58.png)

那么如果我们手头上的资料从来没被分配过，还有办法自动将他们分取有的黑平均算法k means cluster wring，其中所有资料中乱数选择k个中心点，‍‍我们就能把个别资料依照最近的中心点分成k群，将每一群的平均值当成新的k个中心点，‍‍再分成k群，以此类推，最终资料将收敛至k个彼此相近的群体。‍‍以上都是在有历史资料的情形下，利用资料来建构模型的算法

那么如果没有历史资料强化学习，强化学习是在没有历史资料的情况下，把代理人也就是模型直接丢到使用环境当中，透过一连串的动作来观察环境状况，‍‍同时接受来自环境的奖励或惩罚反馈，来动态调整模型，如此一来在经过训练之后，模型就能自动做出能获得最多奖励的动作。‍‍面对这么多琳琅满目的机器学习演算法，我们首先面临的难题就是该套用哪一种演算法。‍‍关于算法的挑选，通常我们会依照用来训练的历史资料有没有标准答案，将演算法分成两大类，至于没有历史资料的强化学习，则独立于这两大类自成一格。‍‍

此外我们也需要考虑每个演算法的特性与前提，假设记录线性回归法是基于特征与结果之间有某种程度的线性关系为前提，会遇到非线性关系的状况就‍‍不太适用。‍‍在例如单纯费是分类器，是基于特征之间两两相互独立为前提，到特征之间有相应性的状况，就不太适用以此类推。‍‍除此之外还有许多杂七杂八的因素，像是资料量的大小，模型效能与准确度之间的取舍等等



![截屏2021-10-03 下午3.07.07](/Users/frankmac/Desktop/截屏2021-10-03 下午3.07.07.png)



![截屏2021-10-03 下午3.08.22](/Users/frankmac/Desktop/截屏2021-10-03 下午3.08.22.png)



![截屏2021-10-03 下午3.08.41](/Users/frankmac/Desktop/截屏2021-10-03 下午3.08.41.png)

人脑虽然只有简单的脑神经元组成，却能透过数百到数千亿个神经元之间的相互连接来产生智慧，那么我们能不能用相同的概念让机器去模拟这种普世性的一招打天下的机制，‍‍产生智慧，这个想法开始了神经网络neural networks这个领域，进而演变为后来的深度学习deep learning。‍‍一个大脑神经元有许多数图接收来自其他神经元的动作电位，这些外来动作电位在细胞内进行会诊，‍‍只要电位超过一个阀值，就会触发连锁反应，将神经元的动作电位讯息通过轴突传递给后续的神经元。同理我们可以把大脑神经元的机制，‍‍以是为逻辑的方式来模拟，我们称之为感知器perceptive，其中包含m比输入x一个片子bias，经过权重位相乘并加总之后，‍‍再通过一个激活函数activation functions来模拟大脑神经元的电位发生机制，‍‍最终输出这个节点被激活的程度，传递之下一层的perceptron，因为现实中要解决的难题大多不会有简单的线性级，我们通常会选用非线性函数的FTP选方程，像是介于零与一之间的c‍‍就eave一与一之间的tense，最常被使用的relu或者其他变形，而一旦我们把很多个perceptron分成相互连接起来，‍‍就形成一个differently的模型架构了，要训练这个模型，就把资料一笔一笔未进去前进行正向传播，for population将得出的输出结果与标准答案带入损失函数，‍‍function分出两者之间的差异，在国内点design之类的最佳话，函数after my者进行反向传播，只要资料量过多，模型输出与标准答案之间的差异，就会在资料一笔一笔正向反向流入模型的自我修正当中逐渐收敛减小，一旦进入模型得出的答案与标准答案的差异小到某个可以‍‍接受的程度，就表示这个模型是训练好的可用的模型，这样的概念看似简单，但要实现出来则需要大量的资料，大量的运算能力以及够简单好用的软体。‍‍

![截屏2021-10-03 下午3.09.03](/Users/frankmac/Desktop/截屏2021-10-03 下午3.09.03.png)



![截屏2021-10-03 下午3.09.22](/Users/frankmac/Desktop/截屏2021-10-03 下午3.09.22.png)

也因此在2012年之后，当这三个条件都满足了，deep learning才终于开花结果，开始有了爆炸性的成长。‍‍在电脑视觉领域我们可以使用卷积、神经网络、convolutional network，‍‍通过检测CNN进入小范围的滤镜filter来取得影像的边缘形状等等特征，再把这些富有意义的特征接到前面提到t端的模型，如此就能有效识别图片或影像中的物体，通过‍‍这样的方式，电脑在影响便是正确率上已经超越人。

GAN，通过两个d m的模型相互抗衡，由励志要成为模仿大师的生成模型，generator产生假资料将有判别模型discriminator来判断资料真假，‍‍但生成模型产生出来的假资料，让他们模型分不清真假就成功了。‍‍一些变脸应用的AP是AI生成的画作都是gained的相关应用



![截屏2021-10-03 下午3.10.11](/Users/frankmac/Desktop/截屏2021-10-03 下午3.10.11.png)



![截屏2021-10-03 下午3.10.27](/Users/frankmac/Desktop/截屏2021-10-03 下午3.10.27.png)



![截屏2021-10-03 下午3.10.56](/Users/frankmac/Desktop/截屏2021-10-03 下午3.10.56.png)

针对声音或文字等等自然语言处理，natural language processing简称NLP，这类有顺序性资料的处理，传统上可以使用递归神经网络recurrent neural network，‍‍简称**rnn**把每次训练的模型状态传递之下，一次训练以达到有顺序性的短期记忆的功效。‍‍进阶版的长短期记忆神经网络，no short term memory huo简称**LSTM**，则用于改善RM的长期机遇递减效应，针对类似的问题，后来有人提出另一套更有效率的解法，‍‍成为transformer概念上是使用注意力attention的机制，那模型直接针对重点部分进行处理，这样的机制不只是用于自然语言处理，它们在电脑视觉领域上也有不错的成果。‍‍

![截屏2021-10-03 下午3.11.18](/Users/frankmac/Desktop/截屏2021-10-03 下午3.11.18.png)



围棋领域中结合depth learning，real enforcement learning的AlphaGo，以3:0击败世界第一位骑士科学，这全世界全世界等同宣告ei已经能通过快速自我学习，‍‍在特定领域超越人类数千年以来的智慧累积，2020年288的研发团队demand再度运用deep learning破解了困扰着生物学50年的蛋白质分子折叠问题。‍‍刚刚实际的帮助人类理解疾病机制，促进新药开发，帮助农业生产，进而运用蛋白质来改善地球生态环境，‍‍更贴近生活的自驾车的发展更是不在话下。‍‍当前的自驾车技术随着累积里程数持续增加和趋于成熟，造势率也早已远低于人类，‍‍同时培养在医学领域某些科比的诊断正确率也已经达到优于人类的水准。‍‍明明等了，‍‍至于无人商店与中国天网则早已不是那么新奇的话题了，这时再回头来看1950年处理的问题，机器人思考吗？我们可能还是无法给出一个明确的答案对吧？‍‍当下的人类却已经比当年拥有更多的技术累积成果，更接近这个梦想，并持续前进当中，当前的AI技术就像一个学习成长中的小孩能看能听能说，‍‍已经能针对特定问题做出精准。‍